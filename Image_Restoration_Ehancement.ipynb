{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Restoration-Ehancement.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcDbk4Y5x2aY",
        "outputId": "6ee37b3d-bffe-4635-d2e3-fc955e3ae26d"
      },
      "source": [
        "!pip install -qU flask-ngrok\n",
        "!pip uninstall fastai -y\n",
        "!pip install -qU fastai"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling fastai-1.0.61:\n",
            "  Successfully uninstalled fastai-1.0.61\n",
            "\u001b[K     |████████████████████████████████| 194kB 5.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 5.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 12.8MB 318kB/s \n",
            "\u001b[K     |████████████████████████████████| 776.8MB 22kB/s \n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUACzHAPzZux",
        "outputId": "86a143c0-c857-4fb8-f535-56912c3e2f2c"
      },
      "source": [
        "!python -c \"import torch; print(torch.__version__)\"\n",
        "!python -c \"import fastai; print(fastai.__version__)\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.1\n",
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcKSu7tGhHBt",
        "outputId": "0e7417af-6194-40bf-a9e4-876ca5fb777b"
      },
      "source": [
        "!git clone https://github.com/Vibha27/image-restoration-enhancement.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'image-restoration-enhancement'...\n",
            "remote: Enumerating objects: 132, done.\u001b[K\n",
            "remote: Counting objects: 100% (132/132), done.\u001b[K\n",
            "remote: Compressing objects: 100% (79/79), done.\u001b[K\n",
            "remote: Total 132 (delta 51), reused 118 (delta 42), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (132/132), 5.64 MiB | 6.44 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKvPlxqlRyu2"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import shutil\n",
        "import numpy as np\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from IPython.display import clear_output\n",
        "from flask import Flask, render_template, url_for, request, redirect, abort, send_from_directory"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzCz2KZrQqSZ",
        "cellView": "form"
      },
      "source": [
        "#@title Setup\n",
        "dir = r\"/content/image-restoration-enhancement\"\n",
        "for folder in os.listdir(dir):\n",
        "    src = os.path.join(dir, folder)\n",
        "    if os.path.isdir(src):\n",
        "        shutil.copytree(src, f\"/content/{folder}\")\n",
        "\n",
        "!rm -r /content/static/uploads/*.jpg /content/static/uploads/*.png /content/static/uploads/*.jpeg\n",
        "!rm -r /content/image-restoration-enhancement"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3IGV6GHQF5w",
        "cellView": "form"
      },
      "source": [
        "#@title Model Imports\n",
        "\n",
        "# # esrgan_tf model\n",
        "# # # https://drive.google.com/file/d/14twK6Xm_yuL14eTjQbH3aynXDAXPbMLq/view?usp=sharing\n",
        "!gdown --id 14twK6Xm_yuL14eTjQbH3aynXDAXPbMLq\n",
        "!unzip -qq esrgan_tf.zip\n",
        "!mv /content/content/super_model/esrgan /content/esrgan\n",
        "!rm -r /content/content\n",
        "\n",
        "# # colorization model\n",
        "# # # https://drive.google.com/file/d/1f2M62S-5hf8fO6qCaQ1GNYmA6vrrgiwL/view?usp=sharing\n",
        "!gdown --id 1f2M62S-5hf8fO6qCaQ1GNYmA6vrrgiwL\n",
        "!unzip -q /content/colorisation.zip \n",
        "!mv /content/content/colorisation_model /content/colorisation_model\n",
        "!rm -r /content/content\n",
        "\n",
        "# # Dehazing model\n",
        "\n",
        "!gdown --id 1EGIJADl24Om3Hv7_bJVMPiqU1Dd0kg3b\n",
        "!mkdir dehazing_model\n",
        "!mv /content/ots_train_ffa_3_19.pk /content/dehazing_model/ots_train_ffa_3_19.pk\n",
        "\n",
        "# # Inpainting Model\n",
        "# https://drive.google.com/file/d/19sd-0YiXULvMQ4Wgeja_yDC4X_vqCrYl/view?usp=sharing\n",
        "!gdown --id 19sd-0YiXULvMQ4Wgeja_yDC4X_vqCrYl\n",
        "!unzip -q /content/Image_Inpainting_256_Model.zip\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aev0TmlZnMK5",
        "cellView": "form"
      },
      "source": [
        "#@title Image Super-resolution: Current Tensorflow\n",
        "import tensorflow as tf\n",
        "\n",
        "# MODEL LOADS\n",
        "\n",
        "super_res_model = tf.keras.models.load_model(\"/content/esrgan\")\n",
        "super_res_model.compile=False\n",
        "\n",
        "\n",
        "\n",
        "# SUPER_RES PREPROCESSING TENSORFLOW\n",
        "\n",
        "def preprocess_image(lr_image):\n",
        "    if lr_image.shape[-1] == 4:\n",
        "        lr_image = lr_image[...,:-1]\n",
        "\n",
        "    lr_image = tf.cast(lr_image, tf.float32)\n",
        "\n",
        "    lr_image = tf.convert_to_tensor(\n",
        "        lr_image, dtype=None, dtype_hint=None, name=None\n",
        "    )\n",
        "    return tf.expand_dims(lr_image, 0)\n",
        "    \n",
        "\n",
        "\n",
        "def super_resolution_module(name, file_path, save_dir):\n",
        "    og_image = Image.open(file_path).convert(\"RGB\")\n",
        "    og_image = np.asarray(og_image)\n",
        "    image = preprocess_image(og_image) \n",
        "    fake_image = super_res_model(image)\n",
        "    fake_image = tf.squeeze(fake_image)\n",
        "    fake_image = tf.clip_by_value(fake_image, 0, 255)\n",
        "    fake_image = Image.fromarray(tf.cast(fake_image, tf.uint8).numpy()) \n",
        "    \n",
        "    name = \"out_\"+name\n",
        "    save_path = os.path.join(save_dir, name)\n",
        "    \n",
        "    fake_image.save(save_path)\n",
        "    return save_path\n",
        "\n",
        "# # TEST\n",
        "# super_resolution_module(\"000060.png\", r\"/content/000060.png\", r\"/content/static/OUTPUTS\")\n",
        "clear_output()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jjmEGfPNkiZ",
        "cellView": "form"
      },
      "source": [
        "#@title Image Colorization\n",
        "import torch\n",
        "import fastai\n",
        "from torchvision import transforms\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model_path = r\"/content/colorisation_model/net_G_color.pth\"\n",
        "color_model = torch.load(model_path)\n",
        "color_model.eval()\n",
        "color_model.to(device)\n",
        "\n",
        "\n",
        "def rgb2gray(rgb):\n",
        "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n",
        "\n",
        "def lab_to_rgb(L, ab):\n",
        "    \"\"\"\n",
        "    Takes a batch of images\n",
        "    \"\"\"\n",
        "\n",
        "    L = (L + 1.0) * 50.0\n",
        "    ab = ab * 110.0\n",
        "    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n",
        "\n",
        "    rgb_imgs = []\n",
        "    gray_scales = []\n",
        "\n",
        "    for img in Lab:\n",
        "        img_rgb = lab2rgb(img)\n",
        "        gray_scale = rgb2gray(img_rgb)\n",
        "        # gray_scale = np.expand_dims(gray_scale)\n",
        "        rgb_imgs.append(img_rgb)\n",
        "\n",
        "        gray_scales.append(gray_scale)\n",
        "    return np.clip(np.concatenate(gray_scales, axis=1), 0, 255), torch.from_numpy(np.clip(np.concatenate(rgb_imgs, axis=1), 0, 255))\n",
        "\n",
        "def colorisation_module(name, file_path, save_dir):\n",
        "    img = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    og_h, og_w = img.shape[:2]\n",
        "\n",
        "    img = cv2.resize(img, (256, 256), cv2.INTER_CUBIC)\n",
        "    img = np.asarray(img)\n",
        "    img_lab = rgb2lab(img).astype(\"float32\") # Converting RGB to L*a*b\n",
        "    img_lab = transforms.ToTensor()(img_lab)\n",
        "    L = img_lab[[0], ...] / 50. - 1. # Between -1 and 1\n",
        "    L = L.unsqueeze(0)\n",
        "    L = L.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = color_model(L)\n",
        "    \n",
        "    L, output = lab_to_rgb(L, output)\n",
        "    output = output.numpy() \n",
        "\n",
        "    output = Image.fromarray(np.uint8(output*255))\n",
        "    output = output.resize((og_w, og_h), PIL.Image.BICUBIC)\n",
        "\n",
        "    # print(output.size, type(output), np.max(output), np.min(output))\n",
        "\n",
        "    save_path = os.path.join(save_dir, f\"out_{name}\")\n",
        "    # print(save_path)\n",
        "    output.save(save_path)\n",
        "    # print(output.size, type(output))\n",
        "\n",
        "    return save_path"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y028_krYNkmp",
        "cellView": "form"
      },
      "source": [
        "#@title Image Dehazing \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "def default_conv(in_channels, out_channels, kernel_size, bias=True):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size, padding=(kernel_size//2), bias=bias)\n",
        "    \n",
        "    \n",
        "class PALayer(nn.Module):\n",
        "    def __init__(self, channel):\n",
        "        super(PALayer, self).__init__()\n",
        "        self.pa = nn.Sequential(\n",
        "                nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(channel // 8, 1, 1, padding=0, bias=True),\n",
        "                nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        y = self.pa(x)\n",
        "        return x * y\n",
        "\n",
        "    \n",
        "class CALayer(nn.Module):\n",
        "    def __init__(self, channel):\n",
        "        super(CALayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.ca = nn.Sequential(\n",
        "                nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(channel // 8, channel, 1, padding=0, bias=True),\n",
        "                nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.avg_pool(x)\n",
        "        y = self.ca(y)\n",
        "        return x * y\n",
        "\n",
        "    \n",
        "class Block(nn.Module):\n",
        "    def __init__(self, conv, dim, kernel_size,):\n",
        "        super(Block, self).__init__()\n",
        "        self.conv1 = conv(dim, dim, kernel_size, bias=True)\n",
        "        self.act1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv(dim, dim, kernel_size, bias=True)\n",
        "        self.calayer = CALayer(dim)\n",
        "        self.palayer = PALayer(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.act1(self.conv1(x))\n",
        "        res = res+x \n",
        "        res = self.conv2(res)\n",
        "        res = self.calayer(res)\n",
        "        res = self.palayer(res)\n",
        "        res += x \n",
        "        return res\n",
        "\n",
        "    \n",
        "class Group(nn.Module):\n",
        "    def __init__(self, conv, dim, kernel_size, blocks):\n",
        "        super(Group, self).__init__()\n",
        "        modules = [Block(conv, dim, kernel_size)  for _ in range(blocks)]\n",
        "        modules.append(conv(dim, dim, kernel_size))\n",
        "        self.gp = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.gp(x)\n",
        "        res += x\n",
        "        return res\n",
        "\n",
        "    \n",
        "class FFA(nn.Module):\n",
        "    def __init__(self,gps,blocks,conv=default_conv):\n",
        "        super(FFA, self).__init__()\n",
        "        self.gps = gps\n",
        "        self.dim = 64\n",
        "        kernel_size = 3\n",
        "        pre_process = [conv(3, self.dim, kernel_size)]\n",
        "        assert self.gps==3\n",
        "        self.g1 = Group(conv, self.dim, kernel_size,blocks=blocks)\n",
        "        self.g2 = Group(conv, self.dim, kernel_size,blocks=blocks)\n",
        "        self.g3 = Group(conv, self.dim, kernel_size,blocks=blocks)\n",
        "        self.ca = nn.Sequential(*[\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(self.dim*self.gps,self.dim//16,1,padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.dim//16, self.dim*self.gps, 1, padding=0, bias=True),\n",
        "            nn.Sigmoid()\n",
        "            ])\n",
        "        self.palayer = PALayer(self.dim)\n",
        "\n",
        "        post_process = [\n",
        "            conv(self.dim, self.dim, kernel_size),\n",
        "            conv(self.dim, 3, kernel_size)]\n",
        "\n",
        "        self.pre = nn.Sequential(*pre_process)\n",
        "        self.post = nn.Sequential(*post_process)\n",
        "\n",
        "    def forward(self, x1):\n",
        "        x = self.pre(x1)\n",
        "        res1 = self.g1(x)\n",
        "        res2 = self.g2(res1)\n",
        "        res3 = self.g3(res2)\n",
        "        w = self.ca(torch.cat([res1,res2,res3],dim=1))\n",
        "        w = w.view(-1,self.gps, self.dim)[:,:,:,None,None]\n",
        "        out = w[:,0,::] * res1 + w[:,1,::] * res2+w[:,2,::] * res3\n",
        "        out = self.palayer(out)\n",
        "        x = self.post(out)\n",
        "        return x + x1\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model_path = r\"/content/dehazing_model/ots_train_ffa_3_19.pk\"\n",
        "\n",
        "ckp = torch.load(model_path, map_location=device)\n",
        "dehaze_net = FFA(gps=3, blocks=19)\n",
        "dehaze_net = nn.DataParallel(dehaze_net)\n",
        "dehaze_net.load_state_dict(ckp['model'])\n",
        "dehaze_net.eval()\n",
        "\n",
        "\n",
        "def dehazing_module(name, file_path, save_dir):\n",
        "    haze = Image.open(file_path)\n",
        "    haze1 = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.64, 0.6, 0.58],std=[0.14,0.15, 0.152])\n",
        "    ])(haze)[None,::]\n",
        "\n",
        "    # haze_no = tfs.ToTensor()(haze)[None,::]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred = dehaze_net(haze1)\n",
        "\n",
        "    ts = torch.squeeze(pred.clamp(0,1).cpu())\n",
        "    print(ts.shape)\n",
        "    # ts = make_grid(ts, nrow=1, normalize=True)\n",
        "    save_path = os.path.join(save_dir, f\"out_{name}\")\n",
        "    save_image(ts, save_path)\n",
        "    return save_path"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlf_wvHJNkpf",
        "cellView": "form"
      },
      "source": [
        "#@title Image Inpainting\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import InputSpec\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "\n",
        "\n",
        "class PConv2D(Conv2D):\n",
        "    def __init__(self, *args, n_channels=3, mono=False, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.input_spec = [InputSpec(ndim=4), InputSpec(ndim=4)]\n",
        "\n",
        "    def build(self, input_shape):        \n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "            \n",
        "        if input_shape[0][channel_axis] is None:\n",
        "            raise ValueError('The channel dimension of the inputs should be defined. Found `None`.')\n",
        "            \n",
        "        self.input_dim = input_shape[0][channel_axis]\n",
        "        \n",
        "        # Image kernel\n",
        "        kernel_shape = self.kernel_size + (self.input_dim, self.filters)\n",
        "        self.kernel = self.add_weight(shape=kernel_shape,\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      name='img_kernel',\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        # Mask kernel\n",
        "        self.kernel_mask = K.ones(shape=self.kernel_size + (self.input_dim, self.filters))\n",
        "\n",
        "        # Calculate padding size to achieve zero-padding\n",
        "        self.pconv_padding = (\n",
        "            (int((self.kernel_size[0]-1)/2), int((self.kernel_size[0]-1)/2)), \n",
        "            (int((self.kernel_size[0]-1)/2), int((self.kernel_size[0]-1)/2)), \n",
        "        )\n",
        "\n",
        "        # Window size - used for normalization\n",
        "        self.window_size = self.kernel_size[0] * self.kernel_size[1]\n",
        "        \n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(shape=(self.filters,),\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        name='bias',\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        \n",
        "        # Both image and mask must be supplied\n",
        "        if type(inputs) is not list or len(inputs) != 2:\n",
        "            raise Exception('PartialConvolution2D must be called on a list of two tensors [img, mask]. Instead got: ' + str(inputs))\n",
        "\n",
        "        # Padding done explicitly so that padding becomes part of the masked partial convolution\n",
        "        images = K.spatial_2d_padding(inputs[0], self.pconv_padding, self.data_format)\n",
        "        masks = K.spatial_2d_padding(inputs[1], self.pconv_padding, self.data_format)\n",
        "\n",
        "        # Apply convolutions to mask\n",
        "        mask_output = K.conv2d(\n",
        "            masks, self.kernel_mask, \n",
        "            strides=self.strides,\n",
        "            padding='valid',\n",
        "            data_format=self.data_format,\n",
        "            dilation_rate=self.dilation_rate\n",
        "        )\n",
        "\n",
        "        # Apply convolutions to image\n",
        "        img_output = K.conv2d(\n",
        "            (images*masks), self.kernel, \n",
        "            strides=self.strides,\n",
        "            padding='valid',\n",
        "            data_format=self.data_format,\n",
        "            dilation_rate=self.dilation_rate\n",
        "        )        \n",
        "\n",
        "        # Calculate the mask ratio on each pixel in the output mask\n",
        "        mask_ratio = self.window_size / (mask_output + 1e-8)\n",
        "\n",
        "        # Clip output to be between 0 and 1\n",
        "        mask_output = K.clip(mask_output, 0, 1)\n",
        "\n",
        "        # Remove ratio values where there are holes\n",
        "        mask_ratio = mask_ratio * mask_output\n",
        "\n",
        "        # Normalize iamge output\n",
        "        img_output = img_output * mask_ratio\n",
        "\n",
        "        # Apply bias only to the image (if chosen to do so)\n",
        "        if self.use_bias:\n",
        "            img_output = K.bias_add(\n",
        "                img_output,\n",
        "                self.bias,\n",
        "                data_format=self.data_format)\n",
        "        \n",
        "        # Apply activations on the image\n",
        "        if self.activation is not None:\n",
        "            img_output = self.activation(img_output)\n",
        "            \n",
        "        return [img_output, mask_output]\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.data_format == 'channels_last':\n",
        "            space = input_shape[0][1:-1]\n",
        "            new_space = []\n",
        "            for i in range(len(space)):\n",
        "                new_dim = conv_output_length(\n",
        "                    space[i],\n",
        "                    self.kernel_size[i],\n",
        "                    padding='same',\n",
        "                    stride=self.strides[i],\n",
        "                    dilation=self.dilation_rate[i])\n",
        "                new_space.append(new_dim)\n",
        "            new_shape = (input_shape[0][0],) + tuple(new_space) + (self.filters,)\n",
        "            return [new_shape, new_shape]\n",
        "        if self.data_format == 'channels_first':\n",
        "            space = input_shape[2:]\n",
        "            new_space = []\n",
        "            for i in range(len(space)):\n",
        "                new_dim = conv_output_length(\n",
        "                    space[i],\n",
        "                    self.kernel_size[i],\n",
        "                    padding='same',\n",
        "                    stride=self.strides[i],\n",
        "                    dilation=self.dilation_rate[i])\n",
        "                new_space.append(new_dim)\n",
        "            new_shape = (input_shape[0], self.filters) + tuple(new_space)\n",
        "            return [new_shape, new_shape]\n",
        "\n",
        "## Reference: https://github.com/keras-team/keras/blob/7a39b6c62d43c25472b2c2476bd2a8983ae4f682/keras/utils/conv_utils.py#L85\n",
        "def conv_output_length(input_length, filter_size,\n",
        "                       padding, stride, dilation=1):\n",
        "    if input_length is None:\n",
        "        return None\n",
        "    assert padding in {'same', 'valid', 'full', 'causal'}\n",
        "    dilated_filter_size = (filter_size - 1) * dilation + 1\n",
        "    if padding == 'same':\n",
        "        output_length = input_length\n",
        "    elif padding == 'valid':\n",
        "        output_length = input_length - dilated_filter_size + 1\n",
        "    elif padding == 'causal':\n",
        "        output_length = input_length\n",
        "    elif padding == 'full':\n",
        "        output_length = input_length + dilated_filter_size - 1\n",
        "    return (output_length + stride - 1) // stride\n",
        "\n",
        "\n",
        "class InpaintingModel_256:\n",
        "  '''\n",
        "  Build UNET like model for image inpaining task.\n",
        "  '''\n",
        "  def prepare_model(self, input_size=(256, 256, 3)):\n",
        "    input_image = keras.layers.Input(input_size)\n",
        "    input_mask = keras.layers.Input(input_size, name='encoder_input')\n",
        "\n",
        "    conv1, mask1, conv2, mask2 = self.__encoder_layer(32, input_image, input_mask, ['conv1', 'conv2']) # 3 -> 32\n",
        "\n",
        "    conv3, mask3, conv4, mask4 = self.__encoder_layer(64, conv2, mask2, ['conv3', 'conv4']) # 32 -> 64\n",
        "    conv5, mask5, conv6, mask6 = self.__encoder_layer(64, conv4, mask4, ['conv5', 'conv6']) # 64 -> 64\n",
        "    conv7, mask7, conv8, mask8 = self.__encoder_layer(128, conv6, mask6, ['conv7', 'conv8']) # 64 -> 128\n",
        "\n",
        "    conv9, mask9, conv10, mask10 = self.__encoder_layer(128, conv8, mask8, ['conv9', 'conv10']) # 128 -> 128\n",
        "    conv11, mask11, conv12, mask12 = self.__encoder_layer(256, conv10, mask10, ['conv11', 'conv12']) # 128 -> 256\n",
        "    conv13, mask13, conv14, mask14 = self.__encoder_layer(512, conv12, mask12, ['conv13', 'encoder_output']) # 256 -> 512\n",
        "    # conv15, mask15, conv16, mask16 = self.__encoder_layer(512, conv14, mask14, ['conv15', 'encoder_output']) # 512 -> 512\n",
        "\n",
        "    conv15, mask15, conv16, mask16 = self.__decoder_layer(512, 256, conv14, mask14, conv13, mask13, ['conv15', 'conv16'])\n",
        "    conv17, mask17, conv18, mask18 = self.__decoder_layer(256, 128, conv16, mask16, conv11, mask11, ['conv17', 'conv18'])\n",
        "    conv19, mask19, conv20, mask20 = self.__decoder_layer(128, 128, conv18, mask18, conv9, mask9, ['conv19', 'conv20'])\n",
        "    conv21, mask21, conv22, mask22 = self.__decoder_layer(128, 64, conv20, mask20, conv7, mask7, ['conv21', 'conv22'])\n",
        "    conv23, mask23, conv24, mask24 = self.__decoder_layer(64, 64, conv22, mask22, conv5, mask5, ['conv23', 'conv24'])\n",
        "    conv25, mask25, conv26, mask26 = self.__decoder_layer(64, 32, conv24, mask24, conv3, mask3, ['conv25', 'conv26'])\n",
        "    conv27, mask27, conv28, mask28 = self.__decoder_layer(32, 16, conv26, mask26, conv1, mask1, ['conv27', 'decoder_output'])\n",
        "\n",
        "    # conv29, mask29, conv30, mask30 = self.__decoder_layer(64, 32, conv28, mask28, conv3, mask3, ['conv29', 'conv30'])\n",
        "    # conv31, mask31, conv32, mask32 = self.__decoder_layer(32, 3, conv30, mask30, conv1, mask1, ['conv31', 'decoder_output'])\n",
        "\n",
        "    outputs = keras.layers.Conv2D(3, (1, 1), activation='sigmoid', padding='same')(conv28)\n",
        "\n",
        "    return keras.models.Model(inputs=[input_image, input_mask], outputs=[outputs])\n",
        "\n",
        "    # return keras.models.Model(inputs=[input_image, input_mask], outputs=[conv16])\n",
        "\n",
        "    \n",
        "  def __encoder_layer(self, filters, in_layer, in_mask, names):\n",
        "    conv1, mask1 = PConv2D(32, (3,3), strides=1, padding='same', name=names[0])([in_layer, in_mask])\n",
        "    conv1 = keras.activations.relu(conv1)\n",
        "\n",
        "    conv2, mask2 = PConv2D(32, (3,3), strides=2, padding='same', name=names[1])([conv1, mask1])\n",
        "    conv2 = keras.layers.BatchNormalization(trainable=False)(conv2, training=True)\n",
        "    conv2 = keras.activations.relu(conv2)\n",
        "\n",
        "    return conv1, mask1, conv2, mask2\n",
        "\n",
        "  def __decoder_layer(self, filter1, filter2, in_img, in_mask, share_img, share_mask, names):\n",
        "    up_img = keras.layers.UpSampling2D(size=(2,2))(in_img)\n",
        "    up_mask = keras.layers.UpSampling2D(size=(2,2))(in_mask)\n",
        "    concat_img = keras.layers.Concatenate(axis=3)([share_img, up_img])\n",
        "    concat_mask = keras.layers.Concatenate(axis=3)([share_mask, up_mask])\n",
        "\n",
        "    conv1, mask1 = PConv2D(filter1, (3,3), padding='same', name=names[0])([concat_img, concat_mask])\n",
        "    conv1 = keras.activations.relu(conv1)\n",
        "\n",
        "    conv2, mask2 = PConv2D(filter2, (3,3), padding='same', name=names[1])([conv1, mask1])\n",
        "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
        "    conv2 = keras.activations.relu(conv2)\n",
        "\n",
        "    return conv1, mask1, conv2, mask2\n",
        "\n",
        "model_path = \"/content/inpaitining_model/inpainting_model_best\"\n",
        "\n",
        "inpainting_model = InpaintingModel_256().prepare_model()\n",
        "inpainting_model.load_weights(model_path)\n",
        "inpainting_model.trainable = False"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRERXVxyk4Dq"
      },
      "source": [
        "def inpaint_module(name, file_path, mask_path, save_dir):\n",
        "    IMG_SIZE = 256\n",
        "\n",
        "    mask_image = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
        "    mask_image = cv2.cvtColor(mask_image, cv2.COLOR_BGR2RGB)\n",
        "    og_h, og_w = mask_image.shape[:2]\n",
        "\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_COLOR)\n",
        "    mask_copy = 255 - mask\n",
        "\n",
        "    mask_image = cv2.bitwise_or(mask_copy, mask_image)\n",
        "    # mask_copy = cv2.cvtColor(mask_copy, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    # mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    mask_image = cv2.resize(mask_image, (IMG_SIZE, IMG_SIZE), cv2.INTER_CUBIC)\n",
        "    mask = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), cv2.INTER_CUBIC)\n",
        "\n",
        "    mask_image = np.asarray(mask_image) / 255.0\n",
        "    mask = np.asarray(mask) / 255.0\n",
        "\n",
        "    mask_image = np.expand_dims(mask_image, 0)\n",
        "    mask = np.expand_dims(mask, 0)\n",
        "\n",
        "    # print(mask_image.shape, mask.shape)\n",
        "\n",
        "    inputs = [mask_image, mask]\n",
        "    impainted_image = inpainting_model.predict(inputs)[0]\n",
        "    \n",
        "\n",
        "    # pil_image = Image.fromarray(impainted_image)\n",
        "    impainted_image = Image.fromarray(tf.cast(impainted_image* 255, tf.uint8).numpy() )\n",
        "\n",
        "    # print(impainted_image.shape, type(impainted_image))\n",
        "\n",
        "    impainted_image = impainted_image.resize((og_h, og_w),  Image.BICUBIC)\n",
        "\n",
        "    name = \"out_\"+name\n",
        "    save_path = os.path.join(save_dir, name)\n",
        "    \n",
        "    impainted_image.save(save_path)\n",
        "    return save_path\n",
        "\n",
        "clear_output()\n",
        "\n",
        "# inpaint_module(\"006.png\", \"/content/static/uploads/download.png\", \"/content/static/uploads/masked_download.png\", \"/content/static/uploads\")"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyFu7azENksa"
      },
      "source": [
        ""
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZWz7AcfRwZw",
        "outputId": "98018a82-549a-4261-c106-de0c3b71f2de"
      },
      "source": [
        "# #@title FLASK APP\n",
        "import base64\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "app.config['UPLOAD_EXTENSIONS'] = ['.jpeg', '.png', '.jpg']\n",
        "app.config['UPLOAD_PATH'] = r'/content/static/uploads'\n",
        "app.config['OUTPUT_PATH'] = r\"/content/static/OUTPUTS\"\n",
        "\n",
        "\n",
        "@app.route('/')\n",
        "def index() :\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/upload-image/<pathname>')\n",
        "def uplaod_image(pathname) :\n",
        "    path = request.path\n",
        "    module_name = path.split('/')[-1]\n",
        "    return render_template('upload.html', pathname=module_name)\n",
        "\n",
        "@app.route('/upload-image/<pathname>', methods=['POST'])\n",
        "def upload_files(pathname):\n",
        "    \n",
        "    path = request.path\n",
        "    module_name = path.split('/')[-1]\n",
        "    uploaded_file = request.files['file']\n",
        "    file_name = uploaded_file.filename\n",
        "    file_path = os.path.join(r\"/content/static/uploads\", file_name)\n",
        "    # output_file = request.files['file']\n",
        "\n",
        "    \n",
        "    if file_name != '':\n",
        "        file_ext = os.path.splitext(file_name)[1]\n",
        "        if file_ext not in app.config['UPLOAD_EXTENSIONS'] :\n",
        "            abort(400)\n",
        "\n",
        "        uploaded_file.save(os.path.join(app.config['UPLOAD_PATH'], file_name))\n",
        "        # output_file.save(os.path.join(app.config['OUTPUT_PATH'], filename))\n",
        "        if pathname == 'inpaint' :\n",
        "            image = request.form[\"payload\"].split(',')[1] \n",
        "            img = Image.open(BytesIO(base64.b64decode(image)))\n",
        "            rgb_im = img.convert('RGB')\n",
        "            rgb_im.save(os.path.join(app.config['UPLOAD_PATH'], 'masked_'+file_name))   #saving file to directory\n",
        "\n",
        "        # return filename\n",
        "        # else :\n",
        "        #   return \"<p>404 Not found image. Please upload image</p>\"\n",
        "    \n",
        "    print(module_name, file_path)\n",
        "    if module_name == \"super-resolution\":\n",
        "        if os.path.exists(file_path):\n",
        "            _ = super_resolution_module(file_name, file_path, app.config['UPLOAD_PATH'])\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"File: {file_path} not present\")\n",
        "        \n",
        "    elif module_name == \"colorization\":\n",
        "        if os.path.exists(file_path):\n",
        "            _ = colorisation_module(file_name, file_path, app.config['UPLOAD_PATH'])\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"File: {file_path} not present\")\n",
        "    \n",
        "    elif module_name == \"dehaze\":\n",
        "        if os.path.exists(file_path):\n",
        "            _ = dehazing_module(file_name, file_path, app.config['UPLOAD_PATH'])\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"File: {file_path} not present\")\n",
        "        \n",
        "    \n",
        "    elif module_name == \"inpaint\":\n",
        "        if os.path.exists(file_path):\n",
        "            _ = inpaint_module(file_name, \n",
        "                               file_path, \n",
        "                               os.path.join(app.config['UPLOAD_PATH'], f'masked_{file_name}'), \n",
        "                               app.config['UPLOAD_PATH']\n",
        "            )\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"File: {file_path} not present\")\n",
        "        pass\n",
        "        \n",
        "\n",
        "    return redirect(url_for('uploaded_file', \n",
        "                            pathname=pathname,\n",
        "                            filename= 'masked_'+file_name if pathname == \"inpaint\" else file_name)\n",
        "    )  #calling uploaded_file function\n",
        "    \n",
        "        \n",
        "@app.route('/uploads/<filename>')\n",
        "def send_file(filename):\n",
        "    # print(\"SEND FILE\")\n",
        "    return send_from_directory(app.config['UPLOAD_PATH'], filename)\n",
        "\n",
        "@app.route('/uploads/<filename>')\n",
        "def send_output_file(filename):\n",
        "    print(\"Found Rendering\")\n",
        "    return send_from_directory(r\"/content/static/OUTPUTS\", filename)\n",
        "\n",
        "\n",
        "@app.route('/upload-image/<pathname>/<filename>')\n",
        "def uploaded_file(pathname,filename):\n",
        "    # filename = 'http://127.0.0.1:5000/uploads/' + filename\n",
        "    return render_template('upload.html',pathname=pathname,filename=filename)\n",
        "\n",
        "if __name__ == \"__main__\" :\n",
        "    \n",
        "    app.run()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://4fd07f09422d.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [22/Apr/2021 15:30:31] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:30:32] \"\u001b[37mGET /static/css/main.css HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:30:33] \"\u001b[37mGET /static/images/sr.png HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:30:33] \"\u001b[37mGET /static/images/inpaint.png HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:30:33] \"\u001b[37mGET /static/images/dehaze.jpeg HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:30:33] \"\u001b[37mGET /static/images/color-output.png HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:30:33] \"\u001b[37mGET /static/images/sr-output.png HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:30:33] \"\u001b[37mGET /static/images/color.jpeg HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:30:34] \"\u001b[37mGET /static/images/In-paint_1.png HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:30:34] \"\u001b[37mGET /static/images/haze-output.png HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:30:36] \"\u001b[37mGET /upload-image/dehaze HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:30:36] \"\u001b[37mGET /static/js/script.js HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:30:38] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dehaze /content/static/uploads/2_images.jpg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [22/Apr/2021 15:30:51] \"\u001b[32mPOST /upload-image/dehaze HTTP/1.1\u001b[0m\" 302 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 183, 275])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [22/Apr/2021 15:30:51] \"\u001b[37mGET /upload-image/dehaze/2_images.jpg HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:30:52] \"\u001b[37mGET /uploads/2_images.jpg HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:30:52] \"\u001b[37mGET /static/uploads/out_2_images.jpg HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:31:23] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:31:24] \"\u001b[37mGET /static/images/haze-output.png HTTP/1.1\u001b[0m\" 206 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:31:24] \"\u001b[37mGET /static/images/haze-output.png HTTP/1.1\u001b[0m\" 206 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:31:26] \"\u001b[37mGET /upload-image/dehaze HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dehaze /content/static/uploads/colo_2.jpg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [22/Apr/2021 15:31:57] \"\u001b[32mPOST /upload-image/dehaze HTTP/1.1\u001b[0m\" 302 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 291, 173])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [22/Apr/2021 15:31:57] \"\u001b[37mGET /upload-image/dehaze/colo_2.jpg HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:31:58] \"\u001b[37mGET /static/uploads/out_colo_2.jpg HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:31:58] \"\u001b[37mGET /uploads/colo_2.jpg HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:32:14] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:32:38] \"\u001b[37mGET /upload-image/inpaint HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:32:44] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Apr/2021 15:32:46] \"\u001b[37mGET /upload-image/super-resolution HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "super-resolution /content/static/uploads/56ac2-oldman.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKwIf4GEhyry"
      },
      "source": [
        "# !zip -r files.zip /content/static /content/templates"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
